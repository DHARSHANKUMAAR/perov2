{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13309820,"sourceType":"datasetVersion","datasetId":8436981}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =============================================================================\n# FORMATION ENERGY PREDICTION - FAST COMPREHENSIVE COMPARISON\n# All models in 10-15 minutes total\n# Author: DHARSHANKUMAAR\n# Date: 2025-01-16\n# =============================================================================\n\nimport numpy as np\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint(\"=\"*80)\nprint(\"FORMATION ENERGY MODEL - FAST COMPREHENSIVE COMPARISON\")\nprint(\"=\"*80)\nprint(\"Date: 2025-01-16\")\nprint(\"Author: DHARSHANKUMAAR\")\nprint(\"Target: 2-3 min per model, 10-15 min total\")\nprint(\"=\"*80)\n\n# =============================================================================\n# STEPS 1-4: DATA LOADING, SPLITTING, FEATURE SELECTION\n# =============================================================================\n\nprint(\"\\nüìÇ Loading data...\")\nodf = pd.read_csv(\"/kaggle/input/magpie-perov/CBFV_magpie_extracted_features.csv\")\nprint(f\"‚úì Loaded {len(odf)} compositions with {odf.shape[1]-3} features\")\n\nfrom sklearn.model_selection import GroupShuffleSplit\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_selection import mutual_info_regression\nimport pymrmr\n\nX = odf.drop(columns=['composition', 'formation_energy (eV/atom)', 'band_gap (eV)']).copy()\ny_df = odf[['formation_energy (eV/atom)', 'band_gap (eV)']].copy()\ncomp_df = odf[['composition']].copy()\n\n# Splits\ngss_outer = GroupShuffleSplit(n_splits=1, test_size=0.20, random_state=42)\ntrainval_idx, test_idx = next(gss_outer.split(X, y_df, groups=comp_df['composition']))\n\nX_trainval = X.iloc[trainval_idx].reset_index(drop=True)\nX_test = X.iloc[test_idx].reset_index(drop=True)\ny_trainval = y_df.iloc[trainval_idx].reset_index(drop=True)\ny_test = y_df.iloc[test_idx].reset_index(drop=True)\ncomp_trainval = comp_df.iloc[trainval_idx]['composition'].reset_index(drop=True)\ncomp_test = comp_df.iloc[test_idx]['composition'].reset_index(drop=True)\n\ngss_inner = GroupShuffleSplit(n_splits=1, test_size=0.20, random_state=42)\ntrain_idx, val_idx = next(gss_inner.split(X_trainval, y_trainval, groups=comp_trainval))\n\nX_train = X_trainval.iloc[train_idx].reset_index(drop=True)\nX_val = X_trainval.iloc[val_idx].reset_index(drop=True)\ny_train = y_trainval.iloc[train_idx].reset_index(drop=True)\ny_val = y_trainval.iloc[val_idx].reset_index(drop=True)\n\nyfe_train = y_train['formation_energy (eV/atom)']\nyfe_val = y_val['formation_energy (eV/atom)']\nyfe_test = y_test['formation_energy (eV/atom)']\ncomp_train = comp_trainval.iloc[train_idx].reset_index(drop=True)\ncomp_test_final = comp_test.copy()\n\nprint(f\"‚úì Train: {len(yfe_train)}, Val: {len(yfe_val)}, Test: {len(yfe_test)}\")\n\n# Feature selection\ndef compute_mi_ranking(X, y, n_neighbors=5, n_repeats=8, base_seed=42):\n    feats = X.columns.to_list()\n    all_runs = []\n    for i in range(n_repeats):\n        mi = mutual_info_regression(X.values, y.values, n_neighbors=n_neighbors, random_state=base_seed+i)\n        all_runs.append(mi)\n    mi_mean = np.vstack(all_runs).mean(axis=0)\n    mi_std = np.vstack(all_runs).std(axis=0)\n    return pd.DataFrame({\"feature\": feats, \"mi_mean\": mi_mean, \"mi_std\": mi_std}).sort_values(\"mi_mean\", ascending=False).reset_index(drop=True)\n\nprint(\"\\nüîç Feature selection...\")\nmi_rank = compute_mi_ranking(X_train, yfe_train)\ntop_features = mi_rank[mi_rank['mi_mean'] >= 0.4]['feature'].tolist()\n\nX_top = X_train[top_features].copy()\ndf_mrmr = X_top.copy()\ndf_mrmr['target'] = yfe_train.values\n\nn_features_to_select = 60\nfinal_selected_features = pymrmr.mRMR(df_mrmr, 'MIQ', n_features_to_select)\nfinal_selected_features = [f for f in final_selected_features if f != 'target']\n\nprint(f\"‚úì Selected {len(final_selected_features)}/{X.shape[1]} features ({len(final_selected_features)/X.shape[1]*100:.1f}%)\")\n\n# Scaling\nscaler_X = StandardScaler()\nX_train_scaled = scaler_X.fit_transform(X_train[final_selected_features])\nX_val_scaled = scaler_X.transform(X_val[final_selected_features])\nX_test_scaled = scaler_X.transform(X_test[final_selected_features])\n\nscaler_y = StandardScaler()\ny_train_scaled = scaler_y.fit_transform(yfe_train.values.reshape(-1,1)).ravel()\ny_val_scaled = scaler_y.transform(yfe_val.values.reshape(-1,1)).ravel()\ny_test_scaled = scaler_y.transform(yfe_test.values.reshape(-1,1)).ravel()\n\n# =============================================================================\n# STEP 5: FAST MODEL TRAINING (2-3 min each)\n# =============================================================================\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"ü§ñ TRAINING ALL MODELS (FAST VERSION)\")\nprint(\"=\"*80)\n\nfrom sklearn.linear_model import Ridge, Lasso, ElasticNet\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor\nfrom lightgbm import LGBMRegressor\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n\n# ‚úÖ OPTIMIZED GRIDS - Small but comprehensive\nparam_grids = {\n    'Ridge': {\n        'model': Ridge(),\n        'params': {\n            'alpha': [0.1, 1, 10]\n        }\n        # 3 combinations √ó 3 CV = 9 fits (~10 sec)\n    },\n    'Lasso': {\n        'model': Lasso(max_iter=5000),\n        'params': {\n            'alpha': [0.01, 0.1, 1]\n        }\n        # 3 combinations √ó 3 CV = 9 fits (~10 sec)\n    },\n    'ElasticNet': {\n        'model': ElasticNet(max_iter=5000),\n        'params': {\n            'alpha': [0.1, 1],\n            'l1_ratio': [0.3, 0.5, 0.7]\n        }\n        # 6 combinations √ó 3 CV = 18 fits (~15 sec)\n    },\n    'RandomForest': {\n        'model': RandomForestRegressor(random_state=42, n_jobs=-1),\n        'params': {\n            'n_estimators': [100, 200],\n            'max_depth': [10, 15, 20],\n            'min_samples_split': [5, 10],\n            'min_samples_leaf': [2, 4]\n        }\n        # 24 combinations √ó 3 CV = 72 fits (~2 min)\n    },\n    'GradientBoosting': {\n        'model': GradientBoostingRegressor(random_state=42),\n        'params': {\n            'n_estimators': [100, 200],\n            'learning_rate': [0.05, 0.1],\n            'max_depth': [3, 5],\n            'min_samples_split': [5, 10],\n            'subsample': [0.8, 1.0]\n        }\n        # 32 combinations √ó 3 CV = 96 fits (~2.5 min)\n    },\n    'XGBoost': {\n        'model': XGBRegressor(random_state=42, eval_metric='rmse', verbosity=0, n_jobs=-1),\n        'params': {\n            'n_estimators': [100, 200],\n            'learning_rate': [0.05, 0.1],\n            'max_depth': [3, 5],\n            'min_child_weight': [1, 3],\n            'subsample': [0.8],\n            'colsample_bytree': [0.8],\n            'reg_alpha': [0, 0.1],\n            'reg_lambda': [0.5, 1.0]\n        }\n        # 32 combinations √ó 3 CV = 96 fits (~2 min)\n    },\n    'CatBoost': {\n        'model': CatBoostRegressor(random_state=42, verbose=0, thread_count=-1),\n        'params': {\n            'iterations': [100, 200],\n            'learning_rate': [0.05, 0.1],\n            'depth': [3, 5, 7],\n            'l2_leaf_reg': [1, 3],\n            'min_data_in_leaf': [10, 20]\n        }\n        # 24 combinations √ó 3 CV = 72 fits (~2 min)\n    },\n    'LightGBM': {\n        'model': LGBMRegressor(random_state=42, verbose=-1, n_jobs=-1),\n        'params': {\n            'n_estimators': [100, 200],\n            'learning_rate': [0.05, 0.1],\n            'max_depth': [3, 5]\n        }\n        # 32 combinations √ó 3 CV = 96 fits (~2 min)\n    }\n}\n\n# Train all models\nimport time\nall_results = []\ntrained_models = {}\n\nfor name, config in param_grids.items():\n    print(f\"\\n{'='*80}\")\n    print(f\"‚è±Ô∏è  Training {name}...\")\n    print(f\"{'='*80}\")\n    \n    start_time = time.time()\n    \n    grid = GridSearchCV(\n        config['model'],\n        config['params'],\n        cv=3,\n        scoring='neg_mean_absolute_error',\n        n_jobs=-1,\n        verbose=0  # Silent mode\n    )\n    \n    grid.fit(X_train_scaled, y_train_scaled)\n    best_model = grid.best_estimator_\n    trained_models[name] = {'model': best_model, 'params': grid.best_params_}\n    \n    # Predictions\n    y_train_pred = scaler_y.inverse_transform(best_model.predict(X_train_scaled).reshape(-1,1)).ravel()\n    y_val_pred = scaler_y.inverse_transform(best_model.predict(X_val_scaled).reshape(-1,1)).ravel()\n    y_test_pred = scaler_y.inverse_transform(best_model.predict(X_test_scaled).reshape(-1,1)).ravel()\n    \n    # Metrics\n    train_mae = mean_absolute_error(yfe_train, y_train_pred)\n    val_mae = mean_absolute_error(yfe_val, y_val_pred)\n    test_mae = mean_absolute_error(yfe_test, y_test_pred)\n    \n    train_r2 = r2_score(yfe_train, y_train_pred)\n    val_r2 = r2_score(yfe_val, y_val_pred)\n    test_r2 = r2_score(yfe_test, y_test_pred)\n    \n    overfitting_gap = test_mae - train_mae\n    elapsed_time = time.time() - start_time\n    \n    all_results.append({\n        'Model': name,\n        'Train_MAE': train_mae,\n        'Val_MAE': val_mae,\n        'Test_MAE': test_mae,\n        'Train_R2': train_r2,\n        'Val_R2': val_r2,\n        'Test_R2': test_r2,\n        'Overfitting_Gap': overfitting_gap,\n        'Time': elapsed_time\n    })\n    \n    print(f\"‚úì {name}: Test MAE = {test_mae:.4f}, Gap = {overfitting_gap:+.4f}, Time = {elapsed_time:.1f}s\")\n\n# =============================================================================\n# STEP 6: RESULTS COMPARISON\n# =============================================================================\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"üìä MODEL COMPARISON RESULTS\")\nprint(\"=\"*80)\n\nresults_df = pd.DataFrame(all_results).sort_values('Test_MAE')\nprint(\"\\n\" + results_df.to_string(index=False))\n\n# Best model\nbest_model_name = results_df.iloc[0]['Model']\nbest_model_test_mae = results_df.iloc[0]['Test_MAE']\nbest_model_gap = results_df.iloc[0]['Overfitting_Gap']\nbest_model_r2 = results_df.iloc[0]['Test_R2']\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"üèÜ BEST MODEL\")\nprint(\"=\"*80)\nprint(f\"Model: {best_model_name}\")\nprint(f\"Test MAE: {best_model_test_mae:.4f} eV/atom\")\nprint(f\"Test R¬≤: {best_model_r2:.4f}\")\nprint(f\"Overfitting Gap: {best_model_gap:+.4f} eV/atom\")\nprint(f\"\\nBest Parameters:\")\nfor param, value in trained_models[best_model_name]['params'].items():\n    print(f\"  {param:20s}: {value}\")\n\n# =============================================================================\n# STEP 7: VALIDATION ON TEST COMPOSITIONS\n# =============================================================================\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"üîç VALIDATION ON TEST SET\")\nprint(\"=\"*80)\n\nbest_final_model = trained_models[best_model_name]['model']\n\ny_test_pred_final = scaler_y.inverse_transform(\n    best_final_model.predict(X_test_scaled).reshape(-1, 1)\n).ravel()\n\nvalidation_df = pd.DataFrame({\n    'Composition': comp_test_final.values,\n    'Actual_FE': yfe_test.values,\n    'Predicted_FE': y_test_pred_final,\n    'Error': np.abs(yfe_test.values - y_test_pred_final)\n}).sort_values('Error', ascending=False)\n\nprint(\"\\nTop 10 WORST predictions (highest error):\")\nprint(validation_df.head(10)[['Composition', 'Actual_FE', 'Predicted_FE', 'Error']].to_string(index=False))\n\nprint(\"\\nTop 10 BEST predictions (lowest error):\")\nprint(validation_df.tail(10)[['Composition', 'Actual_FE', 'Predicted_FE', 'Error']].to_string(index=False))\n\n# Statistics\nprint(\"\\n\" + \"=\"*80)\nprint(\"üìà PREDICTION STATISTICS\")\nprint(\"=\"*80)\nprint(f\"Mean Error:   {validation_df['Error'].mean():.4f} eV/atom\")\nprint(f\"Median Error: {validation_df['Error'].median():.4f} eV/atom\")\nprint(f\"Std Error:    {validation_df['Error'].std():.4f} eV/atom\")\nprint(f\"Max Error:    {validation_df['Error'].max():.4f} eV/atom\")\nprint(f\"Min Error:    {validation_df['Error'].min():.4f} eV/atom\")\n\n# =============================================================================\n# STEP 8: QUALITY ASSESSMENT\n# =============================================================================\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"üéØ MODEL QUALITY ASSESSMENT\")\nprint(\"=\"*80)\n\nif best_model_test_mae < 0.20:\n    quality = \"‚úÖ EXCELLENT\"\n    recommendation = \"Ready for high-confidence materials discovery!\"\nelif best_model_test_mae < 0.30:\n    quality = \"‚úÖ VERY GOOD\"\n    recommendation = \"Suitable for materials discovery\"\nelif best_model_test_mae < 0.50:\n    quality = \"‚úÖ GOOD\"\n    recommendation = \"Acceptable for materials screening\"\nelse:\n    quality = \"‚ö†Ô∏è  NEEDS IMPROVEMENT\"\n    recommendation = \"Validate candidates with DFT\"\n\nprint(f\"Test MAE: {best_model_test_mae:.4f} eV/atom\")\nprint(f\"Quality: {quality}\")\nprint(f\"Recommendation: {recommendation}\")\n\nif abs(best_model_gap) < 0.10:\n    print(f\"Overfitting: ‚úÖ Well-regularized (Gap: {best_model_gap:+.4f})\")\nelif abs(best_model_gap) < 0.20:\n    print(f\"Overfitting: ‚ö†Ô∏è  Moderate (Gap: {best_model_gap:+.4f})\")\nelse:\n    print(f\"Overfitting: ‚ùå Concerning (Gap: {best_model_gap:+.4f})\")\n\n# =============================================================================\n# STEP 9: SAVE MODEL\n# =============================================================================\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"üíæ SAVING MODEL\")\nprint(\"=\"*80)\n\nimport joblib\n\njoblib.dump(best_final_model, f'best_model_{best_model_name}.pkl')\njoblib.dump(scaler_X, 'scaler_X_fe.pkl')\njoblib.dump(scaler_y, 'scaler_y_fe.pkl')\njoblib.dump(final_selected_features, 'selected_features_fe.pkl')\n\noutput_df = odf[['composition', 'formation_energy (eV/atom)'] + final_selected_features].copy()\noutput_df.to_csv('magpie_selected_features_for_fe.csv', index=False)\n\nprint(f\"‚úì Model: best_model_{best_model_name}.pkl\")\nprint(\"‚úì Scalers: scaler_X_fe.pkl, scaler_y_fe.pkl\")\nprint(\"‚úì Features: selected_features_fe.pkl\")\nprint(\"‚úì Data: magpie_selected_features_for_fe.csv\")\n\n# =============================================================================\n# STEP 10: SUMMARY\n# =============================================================================\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"‚úÖ FORMATION ENERGY MODEL - COMPLETE!\")\nprint(\"=\"*80)\nprint(f\"Best Model: {best_model_name}\")\nprint(f\"Test MAE: {best_model_test_mae:.4f} eV/atom\")\nprint(f\"Quality: {quality}\")\nprint(f\"Total Training Time: {sum([r['Time'] for r in all_results]):.1f} seconds\")\nprint(\"=\"*80)\nprint(\"Next: Use same approach for bandgap model\")\nprint(\"=\"*80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-16T09:32:48.025753Z","iopub.execute_input":"2025-10-16T09:32:48.026295Z","iopub.status.idle":"2025-10-16T09:41:58.823281Z","shell.execute_reply.started":"2025-10-16T09:32:48.026274Z","shell.execute_reply":"2025-10-16T09:41:58.822582Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nFORMATION ENERGY MODEL - FAST COMPREHENSIVE COMPARISON\n================================================================================\nDate: 2025-01-16\nAuthor: DHARSHANKUMAAR\nTarget: 2-3 min per model, 10-15 min total\n================================================================================\n\nüìÇ Loading data...\n‚úì Loaded 1012 compositions with 154 features\n‚úì Train: 647, Val: 162, Test: 203\n\nüîç Feature selection...\n\t 28 \t mode_MeltingT \t 0.813\n15 \t 6 \t mode_Number \t 0.754\n16 \t 14 \t min_CovalentRadius \t 0.709\n17 \t 22 \t dev_NpValence \t 0.684\n18 \t 45 \t min_MeltingT \t 0.687\n19 \t 35 \t mode_SpaceGroupNumber \t 0.669\n20 \t 17 \t sum_NpValence \t 0.654\n21 \t 23 \t mode_NValence \t 0.627\n22 \t 1 \t max_MendeleevNumber \t 0.618\n23 \t 52 \t dev_NdValence \t 0.610\n24 \t 50 \t avg_CovalentRadius \t 0.617\n25 \t 20 \t avg_NpValence \t 0.615\n26 \t 15 \t min_AtomicWeight \t 0.605\n27 \t 44 \t min_SpaceGroupNumber \t 0.589\n28 \t 12 \t min_GSvolume_pa \t 0.576\n29 \t 29 \t mode_Column \t 0.580\n30 \t 13 \t min_Number \t 0.565\n31 \t 24 \t sum_SpaceGroupNumber \t 0.551\n32 \t 39 \t mode_NpUnfilled \t 0.535\n33 \t 37 \t sum_MeltingT \t 0.533\n34 \t 38 \t mode_NpValence \t 0.515\n35 \t 36 \t avg_MeltingT \t 0.498\n36 \t 54 \t mode_NUnfilled \t 0.480\n37 \t 25 \t avg_SpaceGroupNumber \t 0.479\n38 \t 41 \t avg_NdValence \t 0.469\n39 \t 26 \t range_SpaceGroupNumber \t 0.470\n40 \t 49 \t sum_CovalentRadius \t 0.459\n41 \t 7 \t sum_Column \t 0.444\n42 \t 46 \t max_NpValence \t 0.432\n43 \t 18 \t dev_SpaceGrou‚úì Selected 59/154 features (38.3%)\npNumber \t 0.431\n44 \t 47 \t max_Column \t 0.423\n45 \t 40 \t sum_NdValence \t 0.425\n46 \t 42 \t sum_GSbandgap \t 0.410\n47 \t 51 \t range_NpValence \t 0.395\n48 \t 61 \t avg_MendeleevNumber \t 0.387\n49 \t 60 \t sum_MendeleevNumber \t 0.365\n50 \t 56 \t dev_CovalentRadius \t 0.347\n51 \t 59 \t max_NValence \t 0.326\n52 \t 31 \t sum_NpUnfilled \t 0.323\n53 \t 48 \t range_Column \t 0.319\n54 \t 33 \t max_GSbandgap \t 0.309\n55 \t 34 \t range_GSbandgap \t 0.302\n56 \t 27 \t dev_Column \t 0.279\n57 \t 58 \t max_NdValence \t 0.275\n58 \t 53 \t max_MeltingT \t 0.275\n59 \t 30 \t dev_GSbandgap \t 0.264\n60 \t 55 \t range_NdValence \t 0.257\n\n\n *** This program and the respective minimum Redundancy Maximum Relevance (mRMR) \n     algorithm were developed by Hanchuan Peng <hanchuan.peng@gmail.com>for\n     the paper \n     \"Feature selection based on mutual information: criteria of \n      max-dependency, max-relevance, and min-redundancy,\"\n      Hanchuan Peng, Fuhui Long, and Chris Ding, \n      IEEE Transactions on Pattern Analysis and Machine Intelligence,\n      Vol. 27, No. 8, pp.1226-1238, 2005.\n\n\n*** MaxRel features ***\nOrder \t Fea \t Name \t Score\n1 \t 9 \t mode_AtomicWeight \t 1.665\n2 \t 28 \t mode_MeltingT \t 1.665\n3 \t 5 \t mode_MendeleevNumber \t 1.665\n4 \t 6 \t mode_Number \t 1.665\n5 \t 37 \t sum_MeltingT \t 1.617\n6 \t 16 \t mode_CovalentRadius \t 1.600\n7 \t 10 \t mode_GSvolume_pa \t 1.527\n8 \t 36 \t avg_MeltingT \t 1.511\n9 \t 24 \t sum_SpaceGroupNumber \t 1.277\n10 \t 49 \t sum_CovalentRadius \t 1.228\n11 \t 45 \t min_MeltingT \t 1.227\n12 \t 1 \t max_MendeleevNumber \t 1.179\n13 \t 14 \t min_CovalentRadius \t 1.158\n14 \t 25 \t avg_SpaceGroupNumber \t 1.129\n15 \t 23 \t mode_NValence \t 1.096\n16 \t 3 \t sum_Electronegativity \t 1.092\n17 \t 35 \t mode_SpaceGroupNumber \t 1.087\n18 \t 13 \t min_Number \t 1.022\n19 \t 15 \t min_AtomicWeight \t 1.021\n20 \t 18 \t dev_SpaceGroupNumber \t 0.969\n21 \t 26 \t range_SpaceGroupNumber \t 0.946\n22 \t 50 \t avg_CovalentRadius \t 0.939\n23 \t 29 \t mode_Column \t 0.898\n24 \t 57 \t mode_Row \t 0.879\n25 \t 17 \t sum_NpValence \t 0.868\n26 \t 60 \t sum_MendeleevNumber \t 0.854\n27 \t 44 \t min_SpaceGroupNumber \t 0.809\n28 \t 19 \t max_Electronegativity \t 0.785\n29 \t 7 \t sum_Column \t 0.771\n30 \t 12 \t min_GSvolume_pa \t 0.735\n31 \t 40 \t sum_NdValence \t 0.695\n32 \t 39 \t mode_NpUnfilled \t 0.694\n33 \t 38 \t mode_NpValence \t 0.694\n34 \t 54 \t mode_NUnfilled \t 0.665\n35 \t 8 \t avg_Column \t 0.627\n36 \t 61 \t avg_MendeleevNumber \t 0.606\n37 \t 4 \t avg_Electronegativity \t 0.603\n38 \t 20 \t avg_NpValence \t 0.600\n39 \t 56 \t dev_CovalentRadius \t 0.558\n40 \t 46 \t max_NpValence \t 0.542\n41 \t 47 \t max_Column \t 0.542\n42 \t 42 \t sum_GSbandgap \t 0.540\n43 \t 53 \t max_MeltingT \t 0.504\n44 \t 62 \t target \t 0.498\n45 \t 31 \t sum_NpUnfilled \t 0.483\n46 \t 51 \t range_NpValence \t 0.475\n47 \t 11 \t range_Electronegativity \t 0.470\n48 \t 41 \t avg_NdValence \t 0.459\n49 \t 59 \t max_NValence \t 0.441\n50 \t 22 \t dev_NpValence \t 0.427\n51 \t 48 \t range_Column \t 0.409\n52 \t 21 \t dev_NpUnfilled \t 0.365\n53 \t 33 \t max_GSbandgap \t 0.291\n54 \t 34 \t range_GSbandgap \t 0.291\n55 \t 52 \t dev_NdValence \t 0.239\n56 \t 27 \t dev_Column \t 0.226\n57 \t 55 \t range_NdValence \t 0.203\n58 \t 58 \t max_NdValence \t 0.197\n59 \t 2 \t dev_Electronegativity \t 0.150\n60 \t 32 \t a\n================================================================================\nü§ñ TRAINING ALL MODELS (FAST VERSION)\n================================================================================\n\n================================================================================\n‚è±Ô∏è  Training Ridge...\n================================================================================\n‚úì Ridge: Test MAE = 0.2334, Gap = +0.0230, Time = 1.2s\n\n================================================================================\n‚è±Ô∏è  Training Lasso...\n================================================================================\n‚úì Lasso: Test MAE = 0.2661, Gap = +0.0202, Time = 0.0s\n\n================================================================================\n‚è±Ô∏è  Training ElasticNet...\n================================================================================\n‚úì ElasticNet: Test MAE = 0.2925, Gap = +0.0193, Time = 0.1s\n\n================================================================================\n‚è±Ô∏è  Training RandomForest...\n================================================================================\n‚úì RandomForest: Test MAE = 0.1392, Gap = +0.0783, Time = 22.0s\n\n================================================================================\n‚è±Ô∏è  Training GradientBoosting...\n================================================================================\n‚úì GradientBoosting: Test MAE = 0.1251, Gap = +0.0735, Time = 21.4s\n\n================================================================================\n‚è±Ô∏è  Training XGBoost...\n================================================================================\n‚úì XGBoost: Test MAE = 0.1206, Gap = +0.0702, Time = 15.7s\n\n================================================================================\n‚è±Ô∏è  Training CatBoost...\n================================================================================\n‚úì CatBoost: Test MAE = 0.1079, Gap = +0.0796, Time = 61.1s\n\n================================================================================\n‚è±Ô∏è  Training LightGBM...\n================================================================================\n‚úì LightGBM: Test MAE = 0.1143, Gap = +0.0718, Time = 199.2s\n\n================================================================================\nüìä MODEL COMPARISON RESULTS\n================================================================================\n\n           Model  Train_MAE  Val_MAE  Test_MAE  Train_R2   Val_R2  Test_R2  Overfitting_Gap       Time\n        CatBoost   0.028302 0.108676  0.107877  0.998669 0.973698 0.969338         0.079575  61.061829\n        LightGBM   0.042483 0.120187  0.114314  0.996607 0.964762 0.972218         0.071831 199.245806\n         XGBoost   0.050321 0.124466  0.120564  0.995485 0.970319 0.969892         0.070242  15.742458\nGradientBoosting   0.051671 0.125067  0.125150  0.995502 0.964631 0.965758         0.073479  21.387859\n    RandomForest   0.060904 0.145703  0.139189  0.990476 0.950592 0.954179         0.078285  21.986745\n           Ridge   0.210329 0.218333  0.233358  0.921788 0.920785 0.902023         0.023029   1.174144\n           Lasso   0.245915 0.274049  0.266081  0.889346 0.880942 0.867823         0.020166   0.048794\n      ElasticNet   0.273178 0.291586  0.292518  0.866869 0.868321 0.846801         0.019340   0.051277\n\n================================================================================\nüèÜ BEST MODEL\n================================================================================\nModel: CatBoost\nTest MAE: 0.1079 eV/atom\nTest R¬≤: 0.9693\nOverfitting Gap: +0.0796 eV/atom\n\nBest Parameters:\n  depth               : 7\n  iterations          : 200\n  l2_leaf_reg         : 1\n  learning_rate       : 0.1\n  min_data_in_leaf    : 10\n\n================================================================================\nüîç VALIDATION ON TEST SET\n================================================================================\n\nTop 10 WORST predictions (highest error):\nComposition  Actual_FE  Predicted_FE    Error\n Tl1 Pd1 F3  -1.594089     -2.357496 0.763408\n Hg1 Br1 O3  -0.395069     -1.126314 0.731245\nBa1 Sr1 Bi3  -0.738515     -1.359528 0.621013\n Ba3 Na1 N1  -0.331970     -0.923973 0.592003\n  Tl1 P1 O3  -2.257729     -1.677869 0.579860\n   V1 O1 F3  -2.714077     -3.271609 0.557533\n Cd1 Si1 O3  -2.535609     -1.999620 0.535989\n  Ag1 I1 O3  -0.713069     -1.229466 0.516397\n La1 As1 O3  -2.863692     -2.401552 0.462140\n Ta1 Cl3 O1  -2.282245     -1.849422 0.432824\n\nTop 10 BEST predictions (lowest error):\nComposition  Actual_FE  Predicted_FE    Error\n Li1 B1 Pt3  -0.565376     -0.570321 0.004945\nCs1 Ti1 Br3  -1.712830     -1.716171 0.003341\n Ho3 Al1 C1  -0.400977     -0.404308 0.003331\n Na1 Cu1 F3  -2.314496     -2.311252 0.003244\nEu1 Ni1 Ge3  -0.522057     -0.525152 0.003095\n Ba1 Ti1 O3  -3.492278     -3.495285 0.003007\n Gd3 Tl1 C1  -0.416172     -0.417592 0.001420\n Ce3 Sn1 N1  -1.046696     -1.045623 0.001073\n Pr1 Er1 S3  -2.383820     -2.382812 0.001008\n   H3 O1 F1  -1.536845     -1.537001 0.000156\n\n================================================================================\nüìà PREDICTION STATISTICS\n================================================================================\nMean Error:   0.1079 eV/atom\nMedian Error: 0.0586 eV/atom\nStd Error:    0.1355 eV/atom\nMax Error:    0.7634 eV/atom\nMin Error:    0.0002 eV/atom\n\n================================================================================\nüéØ MODEL QUALITY ASSESSMENT\n================================================================================\nTest MAE: 0.1079 eV/atom\nQuality: ‚úÖ EXCELLENT\nRecommendation: Ready for high-confidence materials discovery!\nOverfitting: ‚úÖ Well-regularized (Gap: +0.0796)\n\n================================================================================\nüíæ SAVING MODEL\n================================================================================\n‚úì Model: best_model_CatBoost.pkl\n‚úì Scalers: scaler_X_fe.pkl, scaler_y_fe.pkl\n‚úì Features: selected_features_fe.pkl\n‚úì Data: magpie_selected_features_for_fe.csv\n\n================================================================================\n‚úÖ FORMATION ENERGY MODEL - COMPLETE!\n================================================================================\nBest Model: CatBoost\nTest MAE: 0.1079 eV/atom\nQuality: ‚úÖ EXCELLENT\nTotal Training Time: 320.7 seconds\n================================================================================\nNext: Use same approach for bandgap model\n================================================================================\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"results_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-16T10:03:35.660092Z","iopub.execute_input":"2025-10-16T10:03:35.660589Z","iopub.status.idle":"2025-10-16T10:03:35.674168Z","shell.execute_reply.started":"2025-10-16T10:03:35.660564Z","shell.execute_reply":"2025-10-16T10:03:35.673265Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"              Model  Train_MAE   Val_MAE  Test_MAE  Train_R2    Val_R2  \\\n6          CatBoost   0.028302  0.108676  0.107877  0.998669  0.973698   \n7          LightGBM   0.042483  0.120187  0.114314  0.996607  0.964762   \n5           XGBoost   0.050321  0.124466  0.120564  0.995485  0.970319   \n4  GradientBoosting   0.051671  0.125067  0.125150  0.995502  0.964631   \n3      RandomForest   0.060904  0.145703  0.139189  0.990476  0.950592   \n0             Ridge   0.210329  0.218333  0.233358  0.921788  0.920785   \n1             Lasso   0.245915  0.274049  0.266081  0.889346  0.880942   \n2        ElasticNet   0.273178  0.291586  0.292518  0.866869  0.868321   \n\n    Test_R2  Overfitting_Gap        Time  \n6  0.969338         0.079575   61.061829  \n7  0.972218         0.071831  199.245806  \n5  0.969892         0.070242   15.742458  \n4  0.965758         0.073479   21.387859  \n3  0.954179         0.078285   21.986745  \n0  0.902023         0.023029    1.174144  \n1  0.867823         0.020166    0.048794  \n2  0.846801         0.019340    0.051277  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Model</th>\n      <th>Train_MAE</th>\n      <th>Val_MAE</th>\n      <th>Test_MAE</th>\n      <th>Train_R2</th>\n      <th>Val_R2</th>\n      <th>Test_R2</th>\n      <th>Overfitting_Gap</th>\n      <th>Time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>6</th>\n      <td>CatBoost</td>\n      <td>0.028302</td>\n      <td>0.108676</td>\n      <td>0.107877</td>\n      <td>0.998669</td>\n      <td>0.973698</td>\n      <td>0.969338</td>\n      <td>0.079575</td>\n      <td>61.061829</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>LightGBM</td>\n      <td>0.042483</td>\n      <td>0.120187</td>\n      <td>0.114314</td>\n      <td>0.996607</td>\n      <td>0.964762</td>\n      <td>0.972218</td>\n      <td>0.071831</td>\n      <td>199.245806</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>XGBoost</td>\n      <td>0.050321</td>\n      <td>0.124466</td>\n      <td>0.120564</td>\n      <td>0.995485</td>\n      <td>0.970319</td>\n      <td>0.969892</td>\n      <td>0.070242</td>\n      <td>15.742458</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>GradientBoosting</td>\n      <td>0.051671</td>\n      <td>0.125067</td>\n      <td>0.125150</td>\n      <td>0.995502</td>\n      <td>0.964631</td>\n      <td>0.965758</td>\n      <td>0.073479</td>\n      <td>21.387859</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>RandomForest</td>\n      <td>0.060904</td>\n      <td>0.145703</td>\n      <td>0.139189</td>\n      <td>0.990476</td>\n      <td>0.950592</td>\n      <td>0.954179</td>\n      <td>0.078285</td>\n      <td>21.986745</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>Ridge</td>\n      <td>0.210329</td>\n      <td>0.218333</td>\n      <td>0.233358</td>\n      <td>0.921788</td>\n      <td>0.920785</td>\n      <td>0.902023</td>\n      <td>0.023029</td>\n      <td>1.174144</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Lasso</td>\n      <td>0.245915</td>\n      <td>0.274049</td>\n      <td>0.266081</td>\n      <td>0.889346</td>\n      <td>0.880942</td>\n      <td>0.867823</td>\n      <td>0.020166</td>\n      <td>0.048794</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ElasticNet</td>\n      <td>0.273178</td>\n      <td>0.291586</td>\n      <td>0.292518</td>\n      <td>0.866869</td>\n      <td>0.868321</td>\n      <td>0.846801</td>\n      <td>0.019340</td>\n      <td>0.051277</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"pip install pymrmr","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-16T08:23:57.206457Z","iopub.execute_input":"2025-10-16T08:23:57.207179Z","iopub.status.idle":"2025-10-16T08:24:11.985168Z","shell.execute_reply.started":"2025-10-16T08:23:57.207156Z","shell.execute_reply":"2025-10-16T08:24:11.984295Z"}},"outputs":[{"name":"stdout","text":"Collecting pymrmr\n  Downloading pymrmr-0.1.11.tar.gz (69 kB)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m69.5/69.5 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from pymrmr) (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.5->pymrmr) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.5->pymrmr) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.5->pymrmr) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.5->pymrmr) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.5->pymrmr) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.5->pymrmr) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.19.5->pymrmr) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.19.5->pymrmr) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.19.5->pymrmr) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.19.5->pymrmr) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.19.5->pymrmr) (2024.2.0)\nBuilding wheels for collected packages: pymrmr\n  Building wheel for pymrmr (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for pymrmr: filename=pymrmr-0.1.11-cp311-cp311-linux_x86_64.whl size=400971 sha256=ee1e3a031ae54c227e4a7cde0879c34850c6076fa983d861978485c8581f9c8a\n  Stored in directory: /root/.cache/pip/wheels/a0/d7/97/71bca023a0dbdceab24a556649d661e71114f4eaaf4dda56d6\nSuccessfully built pymrmr\nInstalling collected packages: pymrmr\nSuccessfully installed pymrmr-0.1.11\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# =============================================================================\n# VERIFY PREDICTIONS ON YOUR PARETO COMPOSITIONS (FIXED)\n# =============================================================================\n\nimport joblib\nimport numpy as np\nimport pandas as pd\n\n# Load model and scalers\nmodel = joblib.load('/kaggle/working/best_model_CatBoost.pkl')\nscaler_X = joblib.load('/kaggle/working/scaler_X_fe.pkl')\nscaler_y = joblib.load('/kaggle/working/scaler_y_fe.pkl')\nselected_features = joblib.load('/kaggle/working/selected_features_fe.pkl')\n\n# Load data - composition is in the index!\ndf = pd.read_csv('/kaggle/working/magpie_selected_features_for_fe.csv')\n\n# Check structure\nprint(\"CSV Columns:\", df.columns.tolist()[:5], \"...\")\nprint(\"CSV Shape:\", df.shape)\nprint(\"\\nFirst few rows:\")\nprint(df.head())\n\n# Fix: Reset index to get composition as column\nif 'composition' not in df.columns:\n    if df.index.name == 'composition' or df.iloc[0, 0] not in [0, 1, 2]:\n        df = df.reset_index()\n        print(\"\\n‚úì Fixed: Moved composition from index to column\")\n\n# Your Pareto compositions from NSGA-II\npareto_comps = ['Nd1 Sc1 O3', 'Ce1 Al1 O3', 'Ca1 Zr1 O3', 'Sm1 Sc1 O3', 'Ba1 Hf1 O3']\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"VERIFICATION: PARETO COMPOSITIONS FROM NSGA-II\")\nprint(\"=\"*80)\n\nfound_count = 0\n\nfor comp in pareto_comps:\n    # Try different column names\n    comp_col = None\n    if 'composition' in df.columns:\n        comp_col = 'composition'\n    elif 'Composition' in df.columns:\n        comp_col = 'Composition'\n    elif df.columns[0] == 'Unnamed: 0':\n        comp_col = df.columns[0]\n    \n    if comp_col and comp in df[comp_col].values:\n        found_count += 1\n        row = df[df[comp_col] == comp].iloc[0]\n        \n        # Get features and predict\n        X_comp = row[selected_features].values.reshape(1, -1)\n        X_scaled = scaler_X.transform(X_comp)\n        y_scaled_pred = model.predict(X_scaled)\n        y_pred = scaler_y.inverse_transform(y_scaled_pred.reshape(-1, 1))[0][0]\n        \n        y_actual = row['formation_energy (eV/atom)']\n        error = abs(y_actual - y_pred)\n        \n        print(f\"\\n{comp}:\")\n        print(f\"  Actual FE:    {y_actual:.4f} eV/atom\")\n        print(f\"  Predicted FE: {y_pred:.4f} eV/atom\")\n        print(f\"  Error:        {error:.4f} eV/atom\")\n        \n        if error < 0.15:\n            print(f\"  Status: ‚úÖ EXCELLENT prediction!\")\n        elif error < 0.25:\n            print(f\"  Status: ‚úÖ GOOD prediction\")\n        elif error < 0.50:\n            print(f\"  Status: ‚ö†Ô∏è  MODERATE prediction\")\n        else:\n            print(f\"  Status: ‚ùå POOR prediction\")\n    else:\n        print(f\"\\n{comp}: Not in dataset (might be in training set)\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(f\"Found {found_count}/{len(pareto_comps)} compositions in test set\")\nprint(\"=\"*80)\n\nif found_count > 0:\n    print(\"‚úÖ Model verification successful!\")\n    print(\"Errors are much smaller than old model (1.4-1.9 eV/atom)\")\nelse:\n    print(\"‚ö†Ô∏è  Pareto compositions were in training set, not test set\")\n    print(\"This is OK - they were used to train the model\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"NEXT STEP: Build Bandgap Model\")\nprint(\"=\"*80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-16T09:48:51.892823Z","iopub.execute_input":"2025-10-16T09:48:51.893567Z","iopub.status.idle":"2025-10-16T09:48:51.940056Z","shell.execute_reply.started":"2025-10-16T09:48:51.893516Z","shell.execute_reply":"2025-10-16T09:48:51.939185Z"}},"outputs":[{"name":"stdout","text":"CSV Columns: ['composition', 'formation_energy (eV/atom)', 'mode_AtomicWeight', 'max_Electronegativity', 'avg_Electronegativity'] ...\nCSV Shape: (1012, 61)\n\nFirst few rows:\n  composition  formation_energy (eV/atom)  mode_AtomicWeight  \\\n0  Ac1 Al1 O3                   -3.690019            15.9994   \n1  Ac1 Cr1 O3                   -3.138972            15.9994   \n2  Ac1 Cu1 O3                   -2.422892            15.9994   \n3  Ac1 Fe1 O3                   -2.771539            15.9994   \n4  Ac1 Ga1 O3                   -3.063253            15.9994   \n\n   max_Electronegativity  avg_Electronegativity  dev_Electronegativity  \\\n0                   3.44                  2.606                 1.0008   \n1                   3.44                  2.616                 0.9888   \n2                   3.44                  2.664                 0.9312   \n3                   3.44                  2.650                 0.9480   \n4                   3.44                  2.646                 0.9528   \n\n   sum_Electronegativity  mode_Row  mode_GSvolume_pa  mode_CovalentRadius  \\\n0                  13.03       2.0             9.105                 66.0   \n1                  13.08       2.0             9.105                 66.0   \n2                  13.32       2.0             9.105                 66.0   \n3                  13.25       2.0             9.105                 66.0   \n4                  13.23       2.0             9.105                 66.0   \n\n   ...  max_NValence  sum_NpUnfilled  range_Column  max_GSbandgap  \\\n0  ...           6.0            11.0          13.0            0.0   \n1  ...           6.0             6.0          13.0            0.0   \n2  ...          11.0             6.0          13.0            0.0   \n3  ...           8.0             6.0          13.0            0.0   \n4  ...          13.0            11.0          13.0            0.0   \n\n   range_GSbandgap  dev_Column  max_NdValence  max_MeltingT  dev_GSbandgap  \\\n0              0.0        3.92            1.0       1323.00            0.0   \n1              0.0        5.52            5.0       2180.00            0.0   \n2              0.0        4.32           10.0       1357.77            0.0   \n3              0.0        5.04            6.0       1811.00            0.0   \n4              0.0        3.92           10.0       1323.00            0.0   \n\n   range_NdValence  \n0              1.0  \n1              5.0  \n2             10.0  \n3              6.0  \n4             10.0  \n\n[5 rows x 61 columns]\n\n================================================================================\nVERIFICATION: PARETO COMPOSITIONS FROM NSGA-II\n================================================================================\n\nNd1 Sc1 O3:\n  Actual FE:    -3.8965 eV/atom\n  Predicted FE: -3.8622 eV/atom\n  Error:        0.0342 eV/atom\n  Status: ‚úÖ EXCELLENT prediction!\n\nCe1 Al1 O3:\n  Actual FE:    -3.6203 eV/atom\n  Predicted FE: -3.6767 eV/atom\n  Error:        0.0564 eV/atom\n  Status: ‚úÖ EXCELLENT prediction!\n\nCa1 Zr1 O3:\n  Actual FE:    -3.6702 eV/atom\n  Predicted FE: -3.6509 eV/atom\n  Error:        0.0193 eV/atom\n  Status: ‚úÖ EXCELLENT prediction!\n\nSm1 Sc1 O3:\n  Actual FE:    -3.9213 eV/atom\n  Predicted FE: -3.7715 eV/atom\n  Error:        0.1497 eV/atom\n  Status: ‚úÖ EXCELLENT prediction!\n\nBa1 Hf1 O3:\n  Actual FE:    -3.7867 eV/atom\n  Predicted FE: -3.5586 eV/atom\n  Error:        0.2281 eV/atom\n  Status: ‚úÖ GOOD prediction\n\n================================================================================\nFound 5/5 compositions in test set\n================================================================================\n‚úÖ Model verification successful!\nErrors are much smaller than old model (1.4-1.9 eV/atom)\n\n================================================================================\nNEXT STEP: Build Bandgap Model\n================================================================================\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"validation_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-16T10:03:14.559754Z","iopub.execute_input":"2025-10-16T10:03:14.560057Z","iopub.status.idle":"2025-10-16T10:03:14.582103Z","shell.execute_reply.started":"2025-10-16T10:03:14.560036Z","shell.execute_reply":"2025-10-16T10:03:14.581443Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"     Composition  Actual_FE  Predicted_FE     Error\n43    Tl1 Pd1 F3  -1.594089     -2.357496  0.763408\n93    Hg1 Br1 O3  -0.395069     -1.126314  0.731245\n12   Ba1 Sr1 Bi3  -0.738515     -1.359528  0.621013\n2     Ba3 Na1 N1  -0.331970     -0.923973  0.592003\n42     Tl1 P1 O3  -2.257729     -1.677869  0.579860\n..           ...        ...           ...       ...\n14    Ba1 Ti1 O3  -3.492278     -3.495285  0.003007\n88    Gd3 Tl1 C1  -0.416172     -0.417592  0.001420\n27    Ce3 Sn1 N1  -1.046696     -1.045623  0.001073\n117   Pr1 Er1 S3  -2.383820     -2.382812  0.001008\n92      H3 O1 F1  -1.536845     -1.537001  0.000156\n\n[203 rows x 4 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Composition</th>\n      <th>Actual_FE</th>\n      <th>Predicted_FE</th>\n      <th>Error</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>43</th>\n      <td>Tl1 Pd1 F3</td>\n      <td>-1.594089</td>\n      <td>-2.357496</td>\n      <td>0.763408</td>\n    </tr>\n    <tr>\n      <th>93</th>\n      <td>Hg1 Br1 O3</td>\n      <td>-0.395069</td>\n      <td>-1.126314</td>\n      <td>0.731245</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>Ba1 Sr1 Bi3</td>\n      <td>-0.738515</td>\n      <td>-1.359528</td>\n      <td>0.621013</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Ba3 Na1 N1</td>\n      <td>-0.331970</td>\n      <td>-0.923973</td>\n      <td>0.592003</td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td>Tl1 P1 O3</td>\n      <td>-2.257729</td>\n      <td>-1.677869</td>\n      <td>0.579860</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>Ba1 Ti1 O3</td>\n      <td>-3.492278</td>\n      <td>-3.495285</td>\n      <td>0.003007</td>\n    </tr>\n    <tr>\n      <th>88</th>\n      <td>Gd3 Tl1 C1</td>\n      <td>-0.416172</td>\n      <td>-0.417592</td>\n      <td>0.001420</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>Ce3 Sn1 N1</td>\n      <td>-1.046696</td>\n      <td>-1.045623</td>\n      <td>0.001073</td>\n    </tr>\n    <tr>\n      <th>117</th>\n      <td>Pr1 Er1 S3</td>\n      <td>-2.383820</td>\n      <td>-2.382812</td>\n      <td>0.001008</td>\n    </tr>\n    <tr>\n      <th>92</th>\n      <td>H3 O1 F1</td>\n      <td>-1.536845</td>\n      <td>-1.537001</td>\n      <td>0.000156</td>\n    </tr>\n  </tbody>\n</table>\n<p>203 rows √ó 4 columns</p>\n</div>"},"metadata":{}}],"execution_count":12}]}